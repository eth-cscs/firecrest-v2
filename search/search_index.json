{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["trimmer"]},"docs":[{"location":"","title":"FirecREST v2","text":"<p>FirecREST is an open-source, lightweight REST API for accessing HPC resources developed by the Swiss National Supercomputing Centre (CSCS).</p> <p>FirecREST presents a high performance proxy providing a standardized interface to access HPC infrastructures from the web, with authentication and authorization, supporting multiple schedulers, storages, and filesystems types.</p> <p>Using FirecREST, users and web developers can automate access to HPC resources and create client applications, pipelines, and workflow managers on the top of HPC with a standard and secure interface.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udd10 Authentication and authorization layer integrating Open ID Connect (OIDC)/OAuth2 and OpenFGA</li> <li>\u26a1 High-performance RESTful API powered by asyncIO</li> <li>\u2728 Abstracts underlying HPC technology (schedulers, filesystems, storage, etc.) relying in REST API concept and OpenAPI specification</li> <li>\ud83d\udce1 Async SSH connection pool enabling high-throughput regime</li> <li>\ud83e\ude7a Integrated HPC cluster health checker</li> <li>\ud83d\udca0 Modular architecture with a lightweight and modern stack</li> <li>\ud83d\udcbb Easy to integrate with your code using pyFirecREST Python library</li> </ul>"},{"location":"#get-involved","title":"Get involved","text":"<ul> <li> <p>Check out our GitHub repository, clone the repo, try the Demo environment, create issues, contribute, and more!</p> </li> <li> <p>Join the FirecREST Slack community or contact us via email</p> </li> </ul>"},{"location":"#start-using-firecrest","title":"Start using FirecREST","text":"<ul> <li>Follow the Getting Started documentation to quickly start exploring the features of FirecREST first hand</li> <li>Browse the Use Cases, which brings cases of success applying FirecREST on different tools</li> <li>Check out the OpenAPI specification for the complete set of FirecREST endpoints</li> </ul>"},{"location":"getting_started/","title":"Getting started","text":""},{"location":"getting_started/#quick-start","title":"Quick start","text":"<p>The FirecREST demo allows you to run FirecREST on your laptop or workstation and connect to a functional HPC cluster using your personal credentials.</p> <p>Warning</p> <p>This setup is for illustrative and evaluation purposes only and is not intended for production use.</p>"},{"location":"getting_started/#running-the-firecrest-v2-demo-launcher","title":"Running the FirecREST v2 Demo Launcher","text":"<p>Ensure that Docker is installed and running on your machine, then execute the following command to start the FirecREST v2 demo container:</p> <p>Run FirecREST demo launcher</p> <pre><code>$ docker run -p 8025:8025 -p 5025:5025  -p 3000:3000 --pull always ghcr.io/eth-cscs/firecrest-v2-demo:latest\n</code></pre> <p>Once the container is running, open your browser and navigate to:</p> <p>\u27a1\ufe0f http://localhost:8025/</p>"},{"location":"getting_started/#trying-firecrest-in-a-containerised-environment","title":"Trying FirecREST in a containerised environment","text":"<p>In addition, to test and debug FirecREST locally we provide a local Docker environment that already contains all required dependencies (an HPC cluster, identity provider, object storage, etc.). Please make sure Docker is installed and running on your machine.</p> <p>!!!     To start a local Firecrest environment in VSCode right click the selected environment (e.g. <code>docker-compose.yml</code>) and select \"Compose up\"</p> <p>Alternatively, use the command line:</p> <p>Run development environment on CLI</p> <pre><code>$ git clone https://github.com/eth-cscs/firecrest-v2\n$ cd firecrest-v2\n$ docker compose -f docker-compose.yml up\n</code></pre>"},{"location":"getting_started/#local-environment","title":"Local Environment","text":"<p>The environment comes with an OIDC/OAuth2 Identity Provider (Keycloak) for authentication, a dummy HPC cluster (with Slurm as workload manager) and a storage service (MinIO with S3 interface).</p>"},{"location":"getting_started/#users","title":"Users","text":"<p>The environment comes with two predefined users:</p> <ul> <li>fireuser this account represents an actual user</li> <li>firesrv this account represents a service account </li> </ul>"},{"location":"getting_started/#accessing-firecrest","title":"Accessing Firecrest","text":"<ul> <li>The Firecrest endpoints are accessible at: http://localhost:8000/</li> <li>The Firecrest API swagger: http://localhost:8000/docs and http://localhost:8000/redoc</li> </ul> <p>To access most of the end-points you need an authorization token. Firecrest authorization is based on the standard Oauth2 Client Credentials Flow</p> <p>All local environments come with pre-configured test credentials:</p> <p>Firecrest</p> <pre><code>client: firecrest-test-client\nsecret: wZVHVIEd9dkJDh9hMKc6DTvkqXxnDttk\n</code></pre> <p>To obtain an access token you can either use the Authorize button in the swagger documentation or directly access Keycloak's OAuth2 token end-point:</p> <p>Note</p> <p>We use for these examples curl and jq commands</p> Obtain access token <pre><code>$ CLIENT_ID=\"firecrest-test-client\"\n$ CLIENT_SECRET=\"wZVHVIEd9dkJDh9hMKc6DTvkqXxnDttk\"\n$ access_token$(curl -s -X POST http://localhost:8080/auth/realms/kcrealm/protocol/openid-connect/token \\\n-d \"grant_type=client_credentials\" -d \"client_id=$CLIENT_ID\" -d \"client_secret=$CLIENT_SECRET\" \\\n-H \"Content-Type: application/x-www-form-urlencoded\" -H \"Accept: */*\" | jq --raw-output .\"access_token\")\n$ echo $access_token\neyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJ1Xz.......\n</code></pre>"},{"location":"getting_started/#api-call-examples","title":"API call examples","text":"<p>Note</p> <p>In this dev environment there are 2 clusters for testing: <code>cluster-slurm-ssh</code> and <code>cluster-slurm-api</code>. We will use the first one in this example</p> List <code>$HOME</code> directory content <pre><code>    $ curl -s \"http://localhost:8000/filesystems/cluster-slurm-ssh/ops/ls?path=/home/fireuser\" \\\n    -H \"Authorization: Bearer $access_token\" | jq .\n    {\n        \"output\": [\n            {\n                \"name\": \"tmp\",\n                \"type\": \"d\",\n                \"linkTarget\": null,\n                \"user\": \"fireuser\",\n                \"group\": \"users\",\n                \"permissions\": \"rwxr-xr-x\",\n                \"lastModified\": \"2025-03-18T11:01:43\",\n                \"size\": \"4096\"\n            }\n        ]\n    }\n</code></pre> Submit a job to the workload manager and scheduler <pre><code>curl -X POST -s \"http://localhost:8000/compute/cluster-slurm-ssh/jobs\" \\\n    --json '{\"job\": {\"script\": \"#!/bin/bash -l\\nfor i in {1..100}\\ndo\\necho $i\\nsleep 1\\ndone\", \"working_directory\": \"/home/fireuser\"}}'  \\\n    -H \"Authorization: Bearer $access_token\" -H \"Content-Type: application/json\" | jq .\n{\n    \"jobId\": 5\n}\n</code></pre> Check status of the job <pre><code>curl -X GET -s \"http://localhost:8000/compute/cluster-slurm-ssh/jobs/5\" -H \"Authorization: Bearer $access_token\" -H \"Content-Type: application/json\" | jq .\n{\n    \"jobs\": [\n        {\n        \"jobId\": 5,\n        \"name\": \"Count\",\n        \"status\": {\n            \"state\": \"COMPLETED\",\n            \"stateReason\": \"None\",\n            \"exitCode\": 0,\n            \"interruptSignal\": 0\n        },\n        \"tasks\": [\n            {\n            \"id\": \"5.batch\",\n            \"name\": \"batch\",\n            \"status\": {\n                \"state\": \"COMPLETED\",\n                \"stateReason\": null,\n                \"exitCode\": 0,\n                \"interruptSignal\": 0\n            },\n            \"time\": {\n                \"elapsed\": 101,\n                \"start\": 1742492131,\n                \"end\": 1742492232,\n                \"suspended\": 0,\n                \"limit\": null\n            }\n            }\n        ],\n        \"time\": {\n            \"elapsed\": 101,\n            \"start\": 1742492131,\n            \"end\": 1742492232,\n            \"suspended\": 0,\n            \"limit\": 7200\n        },\n        \"account\": \"staff\",\n        \"allocationNodes\": 1,\n        \"cluster\": \"cluster\",\n        \"group\": \"users\",\n        \"nodes\": \"localhost\",\n        \"partition\": \"part01\",\n        \"priority\": 1,\n        \"killRequestUser\": null,\n        \"user\": \"fireuser\",\n        \"workingDirectory\": \"/home/fireuser\"\n        }\n    ]\n}\n</code></pre>"},{"location":"setup/arch/","title":"Architecture of FirecREST","text":"<p>FirecREST is presented as a simple interface to access HPC resources with an HTTP enabled API. </p> <p>In this chapter it is shown how FirecREST is configured from the architectural point of view to provide this integration with HPC clusters, workload manager and schedulers, authentication, data transfer, etc.</p>"},{"location":"setup/arch/#the-full-picture","title":"The full picture","text":"<p>In the figure below it can be seen the different components of the ecosystem of FirecREST. The API doesn't provide these components, instead, it uses known standards in the industry while providing abstractions to support multiple techonologies.</p> <p></p> <p>The dashed lines represents components that are optional, while the solid lines are mandatory for a proper use of FirecREST.</p>"},{"location":"setup/arch/#1-hpc-systems-and-workload-manager-and-schedulers","title":"(1) HPC Systems and workload manager and schedulers","text":"<p>Check out here how does FirecREST connect to the systems to execute commands and interact with the scheduler and filesystems.</p>"},{"location":"setup/arch/#2-authenticationauthorization","title":"(2) Authentication/Authorization","text":"<p>Review how FirecREST enables access and fine-grained permissions here.</p>"},{"location":"setup/arch/#3-data-transfers","title":"(3) Data Transfers","text":"<p>FirecREST handles data transfer up to 5 TB. See how this is done here.</p>"},{"location":"setup/arch/#4-health-checks","title":"(4) Health checks","text":"<p>Periodically FirecREST will check that the underlying services are healthy.</p>"},{"location":"setup/arch/auth/","title":"Authentication and Authorization on FirecREST","text":"<p>FirecREST relies on industry standards for authentication (AuthN) and authorization (AuthZ), providing easy integration third party auth solutions.</p> <p>Note</p> <p>You can refer to this document to better understand the difference between AuthN and AuthZ.</p>"},{"location":"setup/arch/auth/#authentication","title":"Authentication","text":"<p>FirecREST enables OpenID Connect (OIDC) and OAuth 2.0 as the standard for authentication layer.</p> <p>A JSON Web Tokens (JWT) is sent within the <code>Authorization</code> header bearer token with information related to the user that executes a command on the HPC system.</p> <p>This JWT (also known as \"access token\") works as a credential to access the API instead of using username/password or SSH credentials.</p> <p>This means that the HPC center using FirecREST must be supplied with an Identity Provider (IdP) that supports OIDC/OAuth2 Bearer Tokens for proper use of the API.</p> <p>As IdPs that supports OIDC/OAuth2 we can find Keycloak, ShibbolethIDP5, Auth0, WSO2 Identity Server, among others.</p> <p>Note</p> <p>Note that in order to provide HPC user information in the IdP, this must be somehow synchronized with the user database of the HPC center (ie, LDAP). For instance, in ShibbolethIDP5 it is mandatory to provide a functional LDAP server to start the IdP service.</p>"},{"location":"setup/arch/auth/#token-validation","title":"Token validation","text":"<p>The public key of the IdP must be set on FirecREST configuration to decode the JWT and check the access token has not expired and has been issued by a trusted IdP. This offline validation of the access token can also be complemented by evaluating specific scopes and roles in the token.</p> <p>Note</p> <p>Access token format depends on how the IdP is configured to issue access tokens. FirecREST allows different settings to decrypt and decode the JWT.</p> <p></p> <p>Given that FirecREST solely requires a valid access token from a trusted IdP, therefore the client application consuming FirecREST can use any authorization grant defined in the OAuth2 protocol</p> <ul> <li>Authorization code: used in UI applications (browser, mobile, etc).</li> <li>Implicit</li> <li>Password credentials, and</li> <li>Client credentials</li> </ul> <p>Client credential type is especially important for APIs such as FirecREST, since it enables automated pipelines and workflows on HPC (for instance, a CI/CD pipeline that test a scientific library performs as expected on the HPC system every day at the same time, or automaticaly triggered after a maintainence, etc).</p> <p>You can see a list of use cases where client credentials grant type is crucial to perform the workflow here.</p>"},{"location":"setup/arch/auth/#a-note-on-client-credentials-grant-type","title":"A note on Client Credentials grant type","text":"<p>As mentioned above, client credentials grant type is an important case of authentication method for FirecREST, since it enables machine-to-machine communication, without the need of on-site human intervention with the AuthN process, enabling automated pipelines and cron jobs to run on HPC.</p> <p>A limitation with client credentials is that it's not designed to be used by \"human\" clients but for machines (also known as \"service accounts\" for some IdPs), and thus it doesn't include any information of the user.</p> <p>This integration between users and clients must be done on the HPC center side. FirecREST expects that the access token's <code>preferred_username</code> or <code>username</code> claims are populated with the user name on the target system (see FirecREST command execution docs)</p>"},{"location":"setup/arch/auth/#authorization","title":"Authorization","text":"<p>Note</p> <p>The authorization layer is optional on FirecREST. By default no extra authorization more than validating the access token is done.</p> <p>FirecREST supports fine-grained authorization allowing the community to easily integrate different paradigms within the API.</p> <p>Typical use cases for having an AuthZ layer for HPC access are listed below:</p> <ul> <li> <p>Limit users to access HPC cluster/resources: users can be allowed via LDAP/SSH to use the cluster, however since the API is a different type of access you could limit how the users interface resources with a programmatic layer.</p> </li> <li> <p>Constraint users (or OIDC clients) to access endpoints of the API: a user would like to set its client only for job submissions, another for data transfer, another for both, etc.</p> </li> <li> <p>Quota management: limit the usage of resources (workload schedulers and managers, storage, etc) using FirecREST.</p> </li> </ul>"},{"location":"setup/arch/auth/#firecrest-rebac-authz","title":"FirecREST ReBAC AuthZ","text":"<p>By default, it provides native integration with OpenFGA an open-source tool that uses Relationship Based Access Control (ReBAC) paradigm as authorization solution.</p> <p>The OpenFGA adapter presented on the FirecREST-v2 package works as a client for the AuthZ service, expecting the latter to allow authentication with the same IdP.</p> <p></p> <p>Note</p> <p>FirecREST is flexible enough to allow the community to create adapters that support different approaches and technologies for AuthZ like ReBAC, ABAC, RBAC, etc. We encourage the deployers and users to create pull requests on this repository to extend the usage of the API.</p>"},{"location":"setup/arch/external_storage/","title":"Data Transfer","text":""},{"location":"setup/arch/external_storage/#architecture","title":"Architecture","text":"<p>FirecREST enables users to upload and download large data files of up to 5TB each, utilizing S3 buckets as a data buffer.  Users requesting data uploads or downloads to the HPC infrastructure receive presigned URLs to transfer data to or from the S3 storage.</p> <p>Ownership of buckets and data remains with the FirecREST service account, but FirecREST creates one bucket per user. Each file transferred (uploaded or downloaded) is stored in a unique identified data object into the user's bucket. Data objects within the buckets are retained for a configurable period, managed through S3's lifecycle expiration functionality. This expiration period, expressed in days, can be specified using the <code>bucket_lifecycle_configuration</code> parameter.</p> <p>The S3 storage can be either on-premises or cloud-based. In any case it is required an valid service account having sufficient permissions to handle buckets creation and the generation of presigned URLs.</p>"},{"location":"setup/arch/external_storage/#external-data-upload","title":"External Data Upload","text":"<p>Uploading data from the extern of the HPC infrastructure requires users to apply the multipart upload protocol, therefore the data file shall be divided into parts. The size limit of each part is defined in the response of FirecREST to the upload call. </p> <p>Tha maximum size of the parts can be configured via the <code>max_part_size parameter</code>. The specified value must be lower than S3's maximum limit of 5GB per part and it is applied to any upload data call.</p> <p>The user needs to specify the name of the file to be uploaded, the destination path on the HPC cluster and the size of the file, which allows FirecREST to properly generate the set of presigned URLs for uploading each part. </p> <p>After the user completes the upload process, an already scheduled job transfers the data from the S3 bucket to its final destination on the HPC cluster, typically in a dedicated storage. To enhance performance, this job should be scheduled on a partition capable of supporting concurrent executions to enable parallel data transfers. The implementation of this partition may vary based on the hardware resources available in the cluster. Utilizing dedicated nodes could further improve responsiveness and improve the overall throughput of the data transfer.</p> <p>The diagram below illustrates the sequence of calls required to correctly process an upload.</p> <p></p> <ol> <li>The user calls API resource <code>transfer/upload</code> of endpoint <code>filesystem</code> with the parameters<ul> <li><code>path</code>: destination of the file in the HPC cluster</li> <li><code>fileName</code>: destination name of the file</li> <li><code>fileSize</code>: size of the file to transfer expressed in bytes</li> </ul> </li> <li>FirecREST processes the request and, if valid, generates a dedicated bucket on S3 for the specific user. All further file transfers are conducted within this bucket</li> <li>FirecREST schedules a job on the HPC cluster that waits for the completion of the upload in the S3 bucket</li> <li>FirecREST returns the following information to the user<ul> <li><code>maxPartSize</code>: the maximum size for parts</li> <li><code>partsUploadUrls</code>: one distinct upload URL per part</li> <li><code>completeUploadUrl</code>: the URL to complete the multipart upload in compliancy with S3 protocol</li> <li><code>transferJob</code>: information to track the data transfer job</li> </ul> </li> <li>The user uploads data on applying the multipart protocol with presigned URLs. An S3 object, labeled with the file name and uniquely tagged by a UUID, is created within the user's bucket to receive the file parts and merge them into the final uploaded file. The protocol to be followed by users is implemented through the following steps:<ol> <li>Split the file into parts of maximum size as specified</li> <li>Upload each part with the given URLs, collecting the returned E-Tags for checkdata integrity verification</li> <li>Complete the upload with the given URL, providing the list of the E-Tags</li> </ol> </li> <li>The data transfer job detects the upload completion</li> <li>The data transfer job downloads the incoming data from S3 to the destination specified by the user</li> </ol>"},{"location":"setup/arch/external_storage/#download-data-from-extern","title":"Download Data From Extern","text":"<p>Exporting large data file from the HPC cluster to external systems begins with a user's request to download data. FirecREST returns a presigned URL to access the S3 object and then it schedules a job to upload the data to an S3 object into the user's data bucket. The user must wait until the upload process within the HPC infrastructure is fully complete before accessing the data on S3.</p> <p>To address any potential limitations, FirecREST schedules a job to transfer data to S3 using the multipart upload protocol. This process is entirely transparent to the user and ensures that the S3 object becomes accessible only once the transfer is successfully completed.</p> <p>Once the presigned URL is provided by FirecREST, users can access the S3 bucket without any restrictions. The maximum file size allowed for a single download by S3 accommodates even large exported data files, in a single transfer.</p> <p>The diagram below illustrates the sequence of calls required to correctly process a download.</p> <p></p> <ol> <li>The user calls API resource <code>transfer/download</code> of the <code>filesystem</code> endpoint providing the following parameter<ul> <li><code>path</code>: source of the file in the HPC cluster</li> </ul> </li> <li>FirecREST processes the request and creates a dedicated bucket on S3 for the specific user (the same can be used for file transfer upload).</li> <li>FirecREST schedules the data transfer job and responds to the user with the followng data:<ul> <li><code>downloadUrl</code>: the presigned URL to access the S3 object containing the requested data</li> <li><code>transferJob</code>: information to track the data transfer job</li> </ul> </li> <li>The transfer job generates an S3 object within the user's designated bucket, labeled with the file name and uniquely tagged with a UUID. It automatically transfers data from the specified source into the bucket, utilizing the multipart upload protocol</li> <li>Once the transfer is complete, the user can access the S3 object in the bucket until the expiration period ends</li> </ol> <p>Although single-file download is an option, S3 supports HTTP Range Request, which can be used to parallelly download chunks of a file stored in the S3 bucket.</p>"},{"location":"setup/arch/health_checks/","title":"Health checks","text":"<p>Internally, FirecREST performs a periodic health check of all underlying services that needs to work correctly.</p> <p>These periodic tests not only provide the users with updated information about the current status of the HPC Center infra, but help FirecREST to run faster and more efficiently.</p> <p></p> <p>As you can see in the figure above, when enabled the periodic health check will update the status of</p> <ol> <li>HPC cluster connectivity (via SSH)</li> <li>Workload manager and scheduler (via SSH or API calls, depending the service)</li> <li>Filesystem availability (read only)</li> <li>Object storage availabitity (S3 interface is reachable or not)</li> </ol>"},{"location":"setup/arch/health_checks/#status-reports","title":"Status reports","text":"<p>When consulting the API endpoint <code>/status/systems</code> you can see the status of health checks on the response JSON. The filed <code>servicesHealth</code> is composed of a list of <code>serviceTypes</code> as listed above.</p> <p>The common format includes:</p> <ul> <li><code>serviceType</code>: can be (1) <code>ssh</code>, (2) <code>scheduler</code>, (3) <code>filesystem</code>, and (4) <code>s3</code>.</li> <li><code>lastChecked</code>: timestamp of the last time the periodic test was performed</li> <li><code>latency</code>: is used to establish if the service is healthy or not (depends on the configuration of FirecREST)</li> <li><code>healthy</code>: <code>true</code> or <code>false</code></li> <li><code>message</code>: in case of error, it describes why the probe failed</li> </ul> Health check response for <code>serviceType: ssh</code> <pre><code>{\n    \"systems\":[\n        {\n            \"name\": \"system01\",\n            (...)\n            \"servicesHealth\": [\n                (...)\n                {\n                    \"serviceType\":\"ssh\",\n                    \"lastChecked\":\"2025-04-02T07:48:48.367139Z\",\n                    \"latency\":0.4212830066680908,\n                    \"healthy\":true,\n                    \"message\":null\n                },\n                (...)\n            ]\n        },\n        {\n            \"name\": \"system02\",\n            (...)\n            {\n                \"serviceType\": \"ssh\",\n                \"lastChecked\": \"2025-04-02T07:48:49.082887Z\",\n                \"latency\": 1.1373629570007324,\n                \"healthy\": false,\n                \"message\": \"Too many authentication failures\"\n            },\n        }\n    ]\n}\n</code></pre> Health check response for <code>serviceType: scheduler</code> <pre><code>{\n    \"systems\":[\n        {\n            \"name\": \"system01\",\n            (...)\n            \"servicesHealth\": [\n                {\n                    \"serviceType\":\"scheduler\",\n                    \"lastChecked\":\"2025-04-01T00:01:00.000000Z\",\n                    \"latency\":1.1002883911132812,\n                    \"healthy\":true,\n                    \"message\":null,\n                    \"nodes\": {\n                        \"available\":130,\n                        \"total\":143\n                    }\n                },\n            ]\n        },\n        {\n            \"name\": \"system02\",\n            (...)\n            \"servicesHealth\": [\n                {\n                    {\n                        \"serviceType\":\"scheduler\",\n                        \"lastChecked\":\"2025-04-01T00:01:00.000000Z\",\n                        \"latency\":0.02857375144958496,\n                        \"healthy\":false,\n                        \"message\":\"ClientConnectorError: Cannot connect to host\",\n                        \"nodes\":null\n                    }\n                }\n            ]\n        }\n    ]\n}\n</code></pre> Health check response for <code>serviceType: filesystem</code> <pre><code>{\n    \"systems\":[\n        {\n            \"name\": \"system01\",\n            (...)\n            \"servicesHealth\": [\n                {\n                    \"serviceType\":\"filesystem\",\n                    \"lastChecked\":\"2025-04-01T00:01:00.000000Z\",\n                    \"latency\":1.3535213470458984,\n                    \"healthy\":true,\n                    \"message\":null,\n                    \"path\":\"/path/to/filesystem01\"\n                },\n                {\n                    \"serviceType\": \"filesystem\",\n                    \"lastChecked\":\"2025-04-01T00:01:00.000000Z\",\n                    \"latency\":0.9655811786651611,\n                    \"healthy\":false,\n                    \"message\":\"Too many authentication failures\",\n                    \"path\":\"/path/to/filesystem02\"\n                }\n            ]\n        }\n    ]\n}\n</code></pre> Health check response for <code>serviceType: s3</code> <pre><code>{\n    \"systems\":[\n        {\n            \"name\": \"system01\",\n            (...)\n            \"servicesHealth\": [\n                {\n                    \"serviceType\": \"s3\",\n                    \"lastChecked\":\"2025-04-01T00:01:00.000000Z\",\n                    \"latency\": 0.22784161567687988,\n                    \"healthy\": true,\n                    \"message\": null\n                }\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"setup/arch/health_checks/#improving-command-execution","title":"Improving command execution","text":"<p>Health checks not only provide information about the status of the underlying infrastructure of the HPC center; they enable FirecREST to predict that the command executed in the cluster has a very high probability of being executed as expected.</p> <p>When enabled, FirecREST will prevent execute commands on unhealthy services</p> <p>API call to an unhealthy endpoint</p> <pre><code>$ curl filesystem/system02/ops/ls?path=/path/to/filesystem02\n{\n    \"errorType\": \"error\",\n    \"message\": \"The ssh service for the requested system (zinal) is unhealthy.\",\n    \"data\": null,\n    \"user\": \"firecr01\"\n}\n</code></pre>"},{"location":"setup/arch/systems/","title":"FirecREST command execution","text":""},{"location":"setup/arch/systems/#the-simplistic-approach","title":"The simplistic approach","text":"<p>The goal of FirecREST is to provide an HTTP interface to HPC, this means that the interface FirecREST presents to the users and applications must handle HTTP requests with according parameters, verbs, etc.</p> <p>FirecREST receives these requests and translates them into HPC logic, and after the operations are done (ie, commands executed, data uploaded, etc) it returns an HTTP response that complies with the protocol used.</p> <p></p>"},{"location":"setup/arch/systems/#going-deeper","title":"Going deeper","text":"<p>The command execution from FirecREST API server to the HPC cluster is done via SSH connection using HPC system users' credentials.</p> <p>This means that FirecREST can be installed in any platform or infrastructure (cloud provider, VM, local system) with SSH access to the target system(s) configured. It doesn't need to be installed in the same VPN or VLAN of the system.</p> <p>Info</p> <p>FirecREST doesn't execute commands as <code>root</code> user or using <code>sudo</code> commands. All commands are executed on behalf of the users using the users' credential.</p>"},{"location":"setup/arch/systems/#jwt-to-ssh-delegation","title":"JWT to SSH Delegation","text":"<p>To answer to the question on \"how the SSH credentials are obtained or created\", FirecREST provides an integration with JSON Web Tokens used to authenticate agains the API.</p> <p>FirecREST uses the <code>username</code> or <code>preferred_username</code> claim from the JWT access token created by the Identity Provider (IdP) when the user or application authenticated to use the API.</p> <p>Info</p> <p>This <code>username</code> value on the token must be a valid username on the HPC system, otherwise the SSH credential created on its behalf won't be allowed in the system.</p> <p>The JWT signature is verified using the Identity Provider (IdP) public key, and then the token time validity is checked. If both are successful, the <code>username</code> is extracted and a SSH key is created for the user if there isn't an active connection to the cluster.</p>"},{"location":"setup/arch/systems/#obtaining-ssh-credentials-on-behalf-of-the-user","title":"Obtaining SSH credentials on behalf of the user","text":"<p>FirecREST provides an abstraction to allow different ways of getting SSH credentials for the user. Currently, there are the 2 options available:</p> <p></p> <ol> <li> <p>Using an \"SSH Service\"</p> <p>If the HPC center has an API or web service that translates a JWT into an SSH credential, you can connect FirecREST to this service.</p> <p></p> <p>In this case, the user sends the JWT to FirecREST and it is forwarded to the SSH Service. Therefore, the IdP that generates JWT must be trusted by both FirecREST and the SSH Service.</p> <p>Once the SSH credential is created, FirecREST uses the SSH credential to execute commands.</p> <p>The benefit of this approach is that SSH credentials creation and validation is managed within the specific groups.</p> </li> <li> <p>Using static SSH credentials</p> <p>Additionally, FirecREST provides a way to map user identities in the JWT with SSH credentials by using the firecrest-config.yaml.</p> <p></p> <p>SSH credentials or the users should be managed as secrets and should be updated by the administrator of the FirecREST deployment if they change.</p> </li> </ol>"},{"location":"setup/arch/systems/#ssh-configuration-in-target-systems","title":"SSH Configuration in target systems","text":"<p>Info</p> <p>All these configurations are optional, but strongly suggested to increase security (remember that we are allowing machines to machine communication via SSH, we don't want an SSH DoS!) and provide high througput regime via API</p> <p>It is recommended to set a specific <code>Match</code> section on the Open SSHD configuration for requests from FirecREST:</p> <p>Login node SSHd Config</p> <pre><code>$ cat /etc/ssh/sshd_config\n(...)\n# To accept FirecREST host\nMatch Address {IP_ADDR_or_RANGE}\n    TrustedUserCAKeys /path/to/ssh/key.pub\n    PermitRootLogin no\n    DenyGroups root bin admin sys\n    MaxAuthTries 1\n    AllowTcpForwarding no\n    MaxSessions 2500\n</code></pre>"},{"location":"setup/arch/systems/#ssh-configuration-details","title":"SSH Configuration details","text":"<p>Note</p> <p>You can follow this link to have a more complete understanding on SSH configuration used below</p> <ul> <li> <p><code>Match Address</code>: points the IP address (or range) from where FirecREST server connects to the cluster. This way, this <code>Match</code> block only process SSH connections from FirecREST.</p> </li> <li> <p><code>TrustedUserCAKeys</code>: in case of using an SSH Service this option points to the path where the public key of the Certificate Authority that signs SSH credentials is stored.</p> </li> <li> <p><code>DenyGroups</code>: to avoid these special permission groups from executing commands via FirecREST</p> </li> <li> <p><code>MaxSessions</code>: sessions are a result of (SSH connections x Commands executed). The default value is <code>10</code>, but to allow high throughput regime on commands execution via FirecREST, set this value to a higher number (recommended <code>9999</code> which is the max allowed value).</p> </li> </ul> <p></p>"},{"location":"setup/arch/systems/#how-to-enable-high-throughput-regime-via-firecrest","title":"How to enable High Throughput Regime via FirecREST","text":"<p>To enable high throughput operations on HPC via an API, the FirecREST team has set the SSH connection pool.</p> <p>Instead of using one SSH connection+channel per command, this approach re-uses an SSH connection to execute several commands on a row.\u200b</p> <p>Each connection in the pool is closed after a time of inactivity, leaving it available for the next user.</p> <p></p> <p>Note</p> <p>Using this setup, stress testing has proven that 500 clients can produce ~195 request per second with a latency of 900 ms, and an error rate of 0.04%\u200b (always depending of the infrastructure, filesystems, network latency, etc)</p>"},{"location":"setup/conf/","title":"Configuration Reference","text":"<p>This page documents all available configuration options for FirecREST.</p> <p>Below is an example configuration file showing how values can be structured:</p> Click to view a sample configuration file <pre><code>apis_root_path: \"\"\ndoc_servers:\n  - url: \"http://localhost:8000\"\n    description: \"Local environment\"\nauth:\n  authentication:\n    scopes:  {}\n    tokenUrl:  \"http://keycloak:8080/auth/realms/kcrealm/protocol/openid-connect/token\"\n    publicCerts:\n        - \"http://keycloak:8080/auth/realms/kcrealm/protocol/openid-connect/certs\"\nssh_credentials:\n  fireuser:\n    private_key: \"secret_file:/run/secrets/ssh_private_key_fireuser\"\n  firesrv:\n    private_key: \"secret_file:/run/secrets/ssh_private_key_firesrv\"\n    passphrase: \"secret_file:/run/secrets/ssh_passphrase_firesrv\"\nclusters:\n- name: \"cluster-slurm-api\"\n  ssh:\n    host: \"192.168.240.2\"\n    port: 22\n    max_clients: 500\n    timeout:\n      connection: 5\n      login: 5\n      command_execution: 5\n      idle_timeout: 60\n      keep_alive: 5\n  scheduler:\n    type: \"slurm\"\n    version: \"24.11.0\"\n    api_url: \"http://192.168.240.2:6820\"\n    api_version: \"0.0.42\"\n    timeout: 10\n  service_account:\n    client_id: \"firecrest-health-check\"\n    secret: \"secret_file:/run/secrets/service_account_client_secret\"\n  probing:\n    interval: 120\n    timeout: 10\n    startup_grace_period: 300\n  datatransfer_jobs_directives:\n    - \"#SBATCH --constraint=mc\"\n    - \"#SBATCH --nodes=1\"\n    - \"#SBATCH --time=0-00:15:00\"\n  file_systems:\n    - path: '/home'\n      data_type: 'users'\n      default_work_dir: true\n- name: \"cluster-slurm-ssh\"\n  ssh:\n    host: \"192.168.240.2\"\n    port: 22\n    max_clients: 500\n    timeout:\n      connection: 5\n      login: 5\n      command_execution: 5\n      idle_timeout: 60\n      keep_alive: 5\n  scheduler:\n    type: \"slurm\"\n    version: \"24.11.0\"\n    timeout: 10\n  service_account:\n    client_id: \"firecrest-health-check\"\n    secret: \"secret_file:/run/secrets/service_account_client_secret\"\n  probing:\n    interval: 120\n    timeout: 5\n  datatransfer_jobs_directives:\n    - \"#SBATCH --nodes=1\"\n    - \"#SBATCH --time=0-00:15:00\"\n    - \"#SBATCH --account={account}\"\n  file_systems:\n    - path: '/home'\n      data_type: 'users'\n      default_work_dir: true\nstorage:\n  name: \"s3-storage\"\n  private_url: \"http://192.168.240.19:9000\"\n  public_url: \"http://localhost:9000\"\n  access_key_id: \"storage_access_key\"\n  secret_access_key: \"secret_file:/run/secrets/s3_secret_access_key\"\n  region: \"us-east-1\"\n  ttl: 604800\n  multipart:\n    use_split: false\n    max_part_size: 2147483648 # 2G\n    parallel_runs: 3\n    tmp_folder: \"tmp\"\n  max_ops_file_size: 1048576 # 1M\n  probing:\n    timeout: 10\n</code></pre> <p>In the following tables, you can find all the supported configuration options, along with their types, descriptions, and default values:</p>"},{"location":"setup/conf/#settings","title":"<code>Settings</code>","text":"<p>FirecREST configuration. Loaded from a YAML file.</p> Field Type Description Default <code>app_debug</code> <code>bool</code> Enable debug mode for the FastAPI application. <code>False</code> <code>app_version</code> <code>str</code> Application version string. <code>'2.x.x'</code> <code>apis_root_path</code> <code>str</code> Base path prefix for exposing the APIs. <code>''</code> <code>doc_servers</code> <code>List[</code> <code>dict</code> <code>]</code> <code>|</code> <code>None</code> Optional documentation servers. For completedocumentation see the <code>servers</code> parameter in theFastAPI docs. <code>None</code> <code>auth</code> Auth Authentication and authorization config (OIDC, FGA). <code>(required)</code> <code>ssh_credentials</code> SSHKeysService <code>|</code> <code>Dict[</code> <code>str</code>, SSHUserKeys <code>]</code> SSH keys service or manually defined user keys. More details in this section. <code>(required)</code> <code>clusters</code> <code>List[</code> HPCCluster <code>]</code> List of configured HPC clusters. <code>[]</code> <code>storage</code> Storage <code>|</code> <code>None</code> Storage backend configuration. More details in this section. <code>None</code> <code>logger</code> Logger Logging configuration options. <code>&lt;generated by Logger()&gt;</code> Details of <code>auth</code> (Auth) Details of <code>ssh_credentials</code> (SSHKeysService) Details of <code>ssh_credentials</code> (SSHUserKeys) Details of <code>clusters</code> (HPCCluster) Details of <code>storage</code> (Storage) Details of <code>logger</code> (Logger)"},{"location":"setup/conf/#auth","title":"<code>Auth</code>","text":"<p>Authentication and authorization configuration.</p> Field Type Description Default <code>authentication</code> Oidc OIDC authentication settings. More info in the authentication section. <code>(required)</code> <code>authorization</code> OpenFGA <code>|</code> <code>None</code> Authorization settings via OpenFGA. More info in the authorization section. <code>None</code> Details of <code>authentication</code> (Oidc) Details of <code>authorization</code> (OpenFGA)"},{"location":"setup/conf/#oidc","title":"<code>Oidc</code>","text":"<p>OpenID Connect (OIDC) authentication configuration.</p> Field Type Description Default <code>scopes</code> <code>dict</code> <code>|</code> <code>None</code> Map of OIDC scopes and their purposes. <code>{}</code> <code>token_url</code> <code>str</code> Token endpoint URL for the OIDC provider. This is used to obtain access tokens for the service account that will do the health checks. <code>(required)</code> <code>public_certs</code> <code>List[</code> <code>str</code> <code>]</code> List of URLs for retrieving public certificates. These are used to verify the OIDC token. <code>[]</code>"},{"location":"setup/conf/#openfga","title":"<code>OpenFGA</code>","text":"<p>Authorization settings using OpenFGA.</p> Field Type Description Default <code>url</code> <code>str</code> OpenFGA API base URL. <code>(required)</code> <code>timeout</code> <code>int</code> <code>|</code> <code>None</code> Connection timeout in seconds. When <code>None</code> the timeout is disabled. <code>1</code> <code>max_connections</code> <code>int</code> Max HTTP connections per host. When set to <code>0</code>, there is no limit. <code>100</code>"},{"location":"setup/conf/#sshkeysservice","title":"<code>SSHKeysService</code>","text":"<p>External service for managing SSH keys.</p> Field Type Description Default <code>url</code> <code>str</code> URL of the SSH keys management service. <code>(required)</code> <code>max_connections</code> <code>int</code> Maximum concurrent connections to the service. When set to <code>0</code>, there is no limit. <code>100</code>"},{"location":"setup/conf/#sshuserkeys","title":"<code>SSHUserKeys</code>","text":"<p>SSH key pair configuration for authenticating to remote systems.</p> Field Type Description Default <code>private_key</code> <code>LoadFileSecretStr</code> SSH private key. You can give directly the content or the file path using <code>'secret_file:/path/to/file'</code>. <code>(required)</code> <code>public_cert</code> <code>str</code> <code>|</code> <code>None</code> Optional SSH public certificate. <code>None</code> <code>passphrase</code> <code>LoadFileSecretStr</code> <code>|</code> <code>None</code> Optional passphrase for the private key. You can give directly the content or the file path using <code>'secret_file:/path/to/file'</code>. <code>None</code>"},{"location":"setup/conf/#hpccluster","title":"<code>HPCCluster</code>","text":"<p>Definition of an HPC cluster, including SSH access, scheduling, and     filesystem layout. More info in     the systems' section.</p> Field Type Description Default <code>name</code> <code>str</code> Unique name for the cluster. <code>(required)</code> <code>ssh</code> SSHClientPool SSH configuration for accessing the cluster nodes. <code>(required)</code> <code>scheduler</code> Scheduler Job scheduler configuration. <code>(required)</code> <code>service_account</code> ServiceAccount Service credentials for internal APIs. <code>(required)</code> <code>probing</code> ClusterProbing Probing configuration for monitoring the cluster. <code>(required)</code> <code>file_systems</code> <code>List[</code> FileSystem <code>]</code> List of mounted file systems on the cluster, such as scratch or home directories. <code>[]</code> <code>datatransfer_jobs_directives</code> <code>List[</code> <code>str</code> <code>]</code> Custom scheduler flags passed to data transfer jobs (e.g. <code>-pxfer</code> for a dedicated partition). <code>[]</code> Details of <code>ssh</code> (SSHClientPool) Details of <code>scheduler</code> (Scheduler) Details of <code>service_account</code> (ServiceAccount) Details of <code>probing</code> (ClusterProbing) Details of <code>file_systems</code> (FileSystem)"},{"location":"setup/conf/#sshclientpool","title":"<code>SSHClientPool</code>","text":"<p>SSH connection pool configuration for remote execution.</p> Field Type Description Default <code>host</code> <code>str</code> SSH target hostname. <code>(required)</code> <code>port</code> <code>int</code> SSH port. <code>(required)</code> <code>proxy_host</code> <code>str</code> <code>|</code> <code>None</code> Optional proxy host for tunneling. <code>None</code> <code>proxy_port</code> <code>int</code> <code>|</code> <code>None</code> Optional proxy port. <code>None</code> <code>max_clients</code> <code>int</code> Maximum number of concurrent SSH clients. <code>100</code> <code>timeout</code> SSHTimeouts SSH timeout settings. <code>&lt;generated by SSHTimeouts()&gt;</code> Details of <code>timeout</code> (SSHTimeouts)"},{"location":"setup/conf/#sshtimeouts","title":"<code>SSHTimeouts</code>","text":"<p>Various SSH settings.</p> Field Type Description Default <code>connection</code> <code>int</code> Timeout (seconds) for initial SSH connection. <code>5</code> <code>login</code> <code>int</code> Timeout (seconds) for SSH login/auth. <code>5</code> <code>command_execution</code> <code>int</code> Timeout (seconds) for executing commands over SSH. <code>5</code> <code>idle_timeout</code> <code>int</code> Max idle time (seconds) before disconnecting. <code>60</code> <code>keep_alive</code> <code>int</code> Interval (seconds) for sending keep-alive messages. <code>5</code>"},{"location":"setup/conf/#scheduler","title":"<code>Scheduler</code>","text":"<p>Cluster job scheduler configuration.</p> Field Type Description Default <code>type</code> <code>enum str</code> (Available options: <code>slurm</code>) Scheduler type. <code>(required)</code> <code>version</code> <code>str</code> Scheduler version. <code>(required)</code> <code>api_url</code> <code>str</code> <code>|</code> <code>None</code> REST API endpoint for scheduler. <code>None</code> <code>api_version</code> <code>str</code> <code>|</code> <code>None</code> Scheduler API version. <code>None</code> <code>timeout</code> <code>int</code> <code>|</code> <code>None</code> Timeout in seconds for scheduler communication with the API. <code>10</code>"},{"location":"setup/conf/#serviceaccount","title":"<code>ServiceAccount</code>","text":"<p>Internal service account credentials.</p> Field Type Description Default <code>client_id</code> <code>str</code> Service account client ID. <code>(required)</code> <code>secret</code> <code>LoadFileSecretStr</code> Service account secret token. You can give directly the content or the file path using <code>'secret_file:/path/to/file'</code>. <code>(required)</code>"},{"location":"setup/conf/#clusterprobing","title":"<code>ClusterProbing</code>","text":"<p>Cluster monitoring attributes.</p> Field Type Description Default <code>interval</code> <code>int</code> Interval in seconds between cluster checks. <code>(required)</code> <code>timeout</code> <code>int</code> Maximum time in seconds allowed per check. <code>(required)</code>"},{"location":"setup/conf/#filesystem","title":"<code>FileSystem</code>","text":"<p>Defines a cluster file system and its type.</p> Field Type Description Default <code>path</code> <code>str</code> Mount path for the file system. <code>(required)</code> <code>data_type</code> <code>enum str</code> (Available options: <code>users</code>, <code>store</code>, <code>archive</code>, <code>apps</code>, <code>scratch</code>, <code>project</code>) File system purpose/type. <code>(required)</code> <code>default_work_dir</code> <code>bool</code> Mark this as the default working directory. <code>False</code>"},{"location":"setup/conf/#storage","title":"<code>Storage</code>","text":"<p>Object storage configuration, including credentials, endpoints, and upload behavior.</p> Field Type Description Default <code>name</code> <code>str</code> Name identifier for the storage. <code>(required)</code> <code>private_url</code> <code>str</code> Private/internal endpoint URL for the storage. <code>(required)</code> <code>public_url</code> <code>str</code> Public/external URL for the storage. <code>(required)</code> <code>access_key_id</code> <code>str</code> Access key ID for S3-compatible storage. <code>(required)</code> <code>secret_access_key</code> <code>LoadFileSecretStr</code> Secret access key for storage. You can give directly the content or the file path using <code>'secret_file:/path/to/file'</code>. <code>(required)</code> <code>region</code> <code>str</code> Region of the storage bucket. <code>(required)</code> <code>ttl</code> <code>int</code> Time-to-live (in seconds) for generated URLs. <code>(required)</code> <code>tenant</code> <code>str</code> <code>|</code> <code>None</code> Optional tenant identifier for multi-tenant setups. <code>None</code> <code>multipart</code> MultipartUpload Settings for multipart upload, including chunk size and concurrency. <code>&lt;generated by MultipartUpload()&gt;</code> <code>bucket_lifecycle_configuration</code> BucketLifestyleConfiguration Lifecycle policy settings for auto-deleting files after a given number of days. <code>&lt;generated by BucketLifestyleConfiguration()&gt;</code> <code>max_ops_file_size</code> <code>int</code> Maximum file size (in bytes) allowed for direct upload and download. Larger files will go through the staging area. <code>5242880</code> <code>probing</code> StorageProbing <code>|</code> <code>None</code> Configuration for probing storage availability. <code>None</code> Details of <code>multipart</code> (MultipartUpload) Details of <code>bucket_lifecycle_configuration</code> (BucketLifestyleConfiguration) Details of <code>probing</code> (StorageProbing)"},{"location":"setup/conf/#multipartupload","title":"<code>MultipartUpload</code>","text":"<p>Configuration for multipart upload behavior.</p> Field Type Description Default <code>use_split</code> <code>bool</code> Enable or disable splitting large files into parts when uploading the file to the staging area. <code>False</code> <code>max_part_size</code> <code>int</code> Maximum size (in bytes) for multipart data transfers. Default is 2 GB. <code>2147483648</code> <code>parallel_runs</code> <code>int</code> Number of parts to upload in parallel to the staging area. <code>3</code> <code>tmp_folder</code> <code>str</code> Temporary folder used for storing split parts during upload. <code>'tmp'</code>"},{"location":"setup/conf/#bucketlifestyleconfiguration","title":"<code>BucketLifestyleConfiguration</code>","text":"<p>Configuration for automatic object lifecycle in storage buckets.</p> Field Type Description Default <code>days</code> <code>int</code> Number of days after which objects will expire automatically. <code>10</code>"},{"location":"setup/conf/#storageprobing","title":"<code>StorageProbing</code>","text":"<p>Probing configuration to check availability of the storage system.</p> Field Type Description Default <code>timeout</code> <code>int</code> Timeout for storage health probing in seconds. <code>(required)</code>"},{"location":"setup/conf/#logger","title":"<code>Logger</code>","text":"Field Type Description Default <code>enable_tracing_log</code> <code>bool</code> Enable tracing logs. <code>False</code>"},{"location":"setup/deploy/","title":"FirecREST-v2 Deployment","text":""},{"location":"setup/deploy/#helm-charts","title":"Helm Charts","text":"<p>This repository includes a Helm chart to deploy FirecREST version 2.</p>"},{"location":"setup/deploy/#fetching-the-repository","title":"Fetching the repository","text":"<pre><code>helm repo add firecrest-v2 https://eth-cscs.github.io/firecrest-v2/charts/\nhelm repo update\n</code></pre> <p>The available versions can be listed with <pre><code>helm search repo firecrest-v2/firecrest-api --versions\n</code></pre></p> <p>Deploying the chart <pre><code>helm install --create-namespace &lt;deployment-name&gt; -n &lt;namespace&gt; firecrest-v2/firecrest-api --values values.yaml\n</code></pre></p>"},{"location":"use_cases/","title":"FirecREST use cases","text":"<ul> <li>CI/CD pipeline</li> <li>Workflow orchestrators</li> <li>JupyterHub</li> <li>More...</li> </ul>"},{"location":"use_cases/CI-pipeline/","title":"CI/CD pipeline with FirecREST","text":"<p>The goal of this tutorial is to create a CI/CD pipeline that will run in an HPC system through FirecREST.</p>"},{"location":"use_cases/CI-pipeline/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic python and git knowledge: The task involves very basic Python. Even if you have experience with another programming language, you'll likely find the task manageable.</li> <li>Github account: The CI will utilize resources from your GitHub account, so make sure you have one.</li> <li>Basic CI/CD understanding: Familiarity with basic concepts of Continuous Integration and Continuous Deployment processes is recommended.</li> </ul>"},{"location":"use_cases/CI-pipeline/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Create an OIDC client, if you haven't already.</p> </li> <li> <p>Create a GitHub repository</p> <ul> <li>Copy all the files of the folder docs/use_cases/CI-pipeline in the root folder of your repository.</li> <li>The workflows may be disabled by default in your repo so go ahead and enable them in the \"Actions\" tab of your repository.</li> </ul> </li> <li> <p>Inspect the code that will be tested:     Take a moment to review the code in the <code>mylib</code> folder. This is the code that will be tested in the CI/CD pipeline.</p> <p>Right now there is nothing meaningful there, but you should add your own tests.</p> </li> <li> <p>Configure CI/CD Pipeline:</p> <ul> <li>Open the CI configuration file (<code>.github/workflows/ci.yml</code>) and, with the help of the comments, try to understand the different steps that are already configured. The main change is the last line of and change it to your project on the machine <code>--account=your_project</code>, but depending on the actual platform of your tests you may need to have more adjustments.</li> <li>Set up the secrets that are used in the pipeline in your account. The variables that are needed are <code>F7T_CLIENT_ID</code>, <code>F7T_CLIENT_SECRET</code>, <code>F7T_URL</code> and <code>F7T_TOKEN_URL</code>. You will also need the env variable <code>F7T_SYSTEM_WORKING_DIR</code>, which corresponds to the directory of your tests (probably you would like to point this to <code>$SCRATCH/test_dir</code>).</li> </ul> </li> <li> <p>Review Results:     Once you've configured the pipeline, commit your changes and push them to your GitHub repository.     You can follow the progress of the workflow in the \"Actions\" tab and ensure that the tests ran successfully, and the job was submitted to Daint without issues.</p> </li> <li> <p>Apply this to your own code!     You can use this as a starting point to set up a more meaningful CI for your code.     To test your code you should adapt especially the functions <code>create_batch_script</code> and <code>check_output</code> in the <code>utilities.py</code> file</p> </li> </ol>"},{"location":"use_cases/CI-pipeline/#additional-resources","title":"Additional Resources","text":"<ul> <li>CSCS Developer Portal (only for CSCS users)</li> <li>pyFirecrest documentation</li> <li>How to set up secrets in Github Actions</li> <li>FirecREST v2</li> </ul>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/#authentication","title":"Authentication","text":"<p>FirecREST authentication follows the OpenID Connect (OIDC) standard.</p> <p>To access most endpoints (see the API reference), you must provide a JWT authorization token in the <code>Authorization</code> header:</p> <p>Authorization header</p> <pre><code>Authorization: Bearer &lt;token&gt;\n</code></pre> <p>FirecREST authenticates users by verifying the JWT token\u2019s signature against trusted certificates (see the configuration section). If the JWT token is valid, FirecREST extracts the <code>username</code> or <code>preferred_username</code> claim to establish the user's identity and propagate it downstream (e.g., for SSH authentication).</p> <p>To obtain a JWT token, you need a trusted Identity Provider that supports OAuth2 or OpenID Connect protocols. The FirecREST Docker Compose development environment (see the Getting Started section) includes a preconfigured Keycloak identity provider.</p> <p>There are multiple grant flows available to obtain a JWT token. The most common ones are:</p>"},{"location":"user_guide/#client-credentials-grant","title":"Client Credentials Grant","text":"<p>This grant is used to authenticate an application (client) rather than an individual user. However, since HPC infrastructures typically require usage tracking, it is recommended to create a dedicated client for each user or project and assign a service account owned by the user/project to the client. </p> <p>Important: Using the identity provider to associate a user or project with a client offers a secure and flexible way to map HPC internal users to FirecREST credentials: client credential \u2190 service account \u2190 user/project</p> <p>In this flow, the client submits its <code>client_id</code> and <code>client_secret</code> directly to the authorization server to obtain an access token and a refresh token.</p> <p>Obtain an access token</p> <pre><code>curl --request POST \\\n--url 'http://localhost:8080/auth/realms/kcrealm/protocol/openid-connect/token' \\\n--header 'content-type: application/x-www-form-urlencoded' \\\n--data grant_type=client_credentials \\\n--data client_id=firecrest-test-client \\\n--data client_secret=wZVHVIEd9dkJDh9hMKc6DTvkqXxnDttk\n</code></pre> <p>Note: The above <code>curl</code> command is configured to work with the provided Docker Compose environment. </p> <p>Expected output example</p> <pre><code>{\"access_token\":\"&lt;token&gt;\",\"expires_in\":300,\"token_type\":\"Bearer\",\"scope\":\"firecrest-v2 profile email\"} \n</code></pre>"},{"location":"user_guide/#authorization-code-grant","title":"Authorization Code Grant","text":"<p>This grant is intended for web applications. The user's browser is redirected (HTTP 302) to the authorization server, which handles authentication (e.g., via username/password, two-factor authentication, etc.).</p> <p>After successful authentication, the authorization server redirects the browser back to a pre-registered endpoint in the web application, passing an authorization code. The web application then uses its own credentials (<code>client_id</code> and <code>client_secret</code>) along with the authorization code to request an access token from the authorization server.</p>"},{"location":"user_guide/#api-reference","title":"API Reference","text":""},{"location":"user_guide/#accessing-http-restful-resources","title":"Accessing HTTP RESTful Resources","text":"<p>The FirecREST API follows RESTful design principles, allowing access to the underlying resources through standard HTTP requests.</p> <p>Each request consists of:</p> <ul> <li>Endpoint (URL): The address of the resource being accessed.</li> <li>Method: One of <code>GET</code>, <code>POST</code>, <code>PUT</code>, or <code>DELETE</code>, depending on the action.</li> <li>Headers: Metadata necessary for authorisation.</li> <li>Body: The request payload in JSON format.</li> </ul> <p>Below is a quick overview of the methods:</p> Method Description <code>GET</code> Retrieves resources <code>POST</code> Creates resources <code>PUT</code> Updates resources <code>DELETE</code> Deletes resources <p>The request body format is specific to each call, the full list of available API calls and requests can be found here: API reference.</p>"},{"location":"user_guide/#response-structure","title":"Response Structure","text":"<p>Each FirecREST API response consists of:</p> <ul> <li>Status Code: Indicates the outcome of the request.</li> <li>Headers: Metadata related to the response.</li> <li>Body: The response data in JSON format.</li> </ul> <p>Below is an overview of HTTP status codes and their meanings:</p> Code Category Description 1xx Informational Communicates protocol-level information 2xx Success Indicates the request was successfully processed 3xx Redirection Instructs the client to take additional action 4xx Client Error Indicates an issue with the request sent by the client 5xx Server Error Indicates an issue on the server's side"},{"location":"user_guide/#resource-groups","title":"Resource Groups","text":"<p>FirecREST API endpoints are categorized into three groups:</p> Group URL Prefix Description Status <code>/status/...</code> Provides status information about FirecREST and underlying resources Compute <code>/compute/...</code> Grants access to the job scheduler Filesystem <code>/filesystem/...</code> Provides access to the filesystem"},{"location":"user_guide/#targeting-systems","title":"Targeting Systems","text":"<p>A single FirecREST instance can manage multiple HPC systems. Most endpoints require specifying which system to access by including the system name in the endpoint path.</p> <p>For example:</p> <p>Endpoint path</p> <pre><code>/compute/{system_name}/jobs\n</code></pre> <p>The <code>{system_name}</code> should correspond to the cluster name provided in the FirecREST configuration.  Refer to the configuration section for details.</p>"},{"location":"user_guide/#full-api-endpoints-list","title":"Full API Endpoints List","text":"<p>The complete list of FirecREST API endpoints is available here:  API reference</p>"},{"location":"user_guide/#synchronous-and-asynchronous-calls","title":"Synchronous and Asynchronous Calls","text":"<p>Most FirecREST endpoints operate synchronously, meaning that the invoked operation is completed before a response is provided. All synchronous responses have a fixed timeout of 5 seconds. If the operation cannot be completed within this time limit, an error is returned.</p> <p>A limited set of filesystem-specific operations are executed asynchronously. These calls are non-blocking, and a jobId is returned. It is the user\u2019s responsibility to track the status of the remote job and retrieve the result upon completion.</p> <p>All asynchronous endpoints are located under  <code>/transfer</code> and follow this path structure:</p> <p>Asynchronous transfers endpoint</p> <pre><code>/filesystem/{system_name}/transfer/...\n</code></pre>"},{"location":"user_guide/#file-transfer","title":"File transfer","text":"<p>FirecREST provides two methods for transferring files: - Small files (up to 5MB by default) can be uploaded or downloaded directly. - Large files must first be transferred to a staging storage system (e.g., S3) before being moved to their final location on the HPC filesystem.</p> <p>Small file transfer endpoints:  - <code>/filesystem/{system_name}/ops/download</code>  - <code>/filesystem/{system_name}/ops/upload</code></p> <p>Large file transfer endpoints:  - <code>/filesystem/{system_name}/transfer/download</code>  - <code>/filesystem/{system_name}/transfer/upload</code></p>"},{"location":"user_guide/#downloading-large-files","title":"Downloading Large Files","text":"<p>When requesting a large file download, FirecREST returns a download URL and a jobId. Once the remote job is completed, the user can retrieve the file using the provided URL.</p>"},{"location":"user_guide/#uploading-large-files","title":"Uploading Large Files","text":"<p>For large file uploads, FirecREST provides multi part upload URLs, the number of URLs depends on the file size. The user must split the file accordingly and upload each part to the assigned URL.</p> <p>Once all parts have been uploaded, the user must call the provided complete upload URL to finalize the transfer. After completion, a remote job moves the file from the staging storage to its final destination.</p>"},{"location":"user_guide/#multi-part-upload-example","title":"Multi part upload example","text":"<p>Split your large file into as many parts as provided partsUploadUrls by the <code>/filesystem/{system}/transfer/upload</code> end-point:</p> <p>Split large file to upload</p> <pre><code>$ split -n 7 -d large-file.zip large-file-part-\n</code></pre> <p>Upload each individual part following the correct part order:</p> <p>Upload parts call</p> <pre><code>$ curl 'https://rgw.cscs.ch/firecresttds%3Auser/62ad2cd8-7398-4955-929d-cbfae5088c6a/large-file.zip?uploadId=2~qiT12y-T1Hhl_ELCozIt3ZlLhMoTcmy&amp;partNumber=1&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=GET9Y98HGJARIS4I447Z%2F20250325%2Fcscs-zonegroup%2Fs3%2Faws4_request&amp;X-Amz-Date=20250325T071416Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=d0edacd3fe1d3dc1e38f5d632f7760275cda29e9e41c49548b5da94e47699400' --upload-file large-file-part-00\n</code></pre> <p>Complete the upload by calling the completeUploadUrl:</p> <p>Complete upload call</p> <pre><code>$ curl 'https://rgw.cscs.ch/firecresttds%3Auser/62ad2cd8-7398-4955-929d-cbfae5088c6a/large-file.zip?uploadId=2~qiT12y-T1Hhl_ELCozIt3ZlLhMoTcmy&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=GET9Y98HGJARIS4I447Z%2F20250325%2Fcscs-zonegroup%2Fs3%2Faws4_request&amp;X-Amz-Date=20250325T071416Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=d0edacd3fe1d3dc1e38f5d632f7760275cda29e9e41c49548b5da94e47699400'\n</code></pre>"},{"location":"user_guide/#firecrest-sdk","title":"FirecREST SDK","text":"<p>PyFirecREST is a Python library designed to simplify the implementation of FirecREST clients.</p>"},{"location":"user_guide/#installation","title":"Installation","text":"<p>To install PyFirecREST, run:</p> <p>Install <code>pyfirecrest</code></p> <pre><code>$ python3 -m pip install pyfirecrest\n</code></pre> <p>For more details, visit the official documentation page.</p>"},{"location":"user_guide/#list-files-example","title":"List files example","text":"<p>List files with <code>pyfirecrest</code></p> <pre><code>import firecrest as fc\n\nclass MyAuthorizationClass:\n    def get_access_token(self):\n        return &lt;TOKEN&gt;\n\nclient = fc.v2.Firecrest(firecrest_url=&lt;firecrest_url&gt;, authorization=MyAuthorizationClass())\n\nfiles = client.list_files(\"cluster\", \"/home/test_user\")\nprint(files)\n</code></pre> <p>More examples are available at: pyfirecrest.readthedocs.io</p>"}]}